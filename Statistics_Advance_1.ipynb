{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe21f73-7e29-4ec3-ade6-3863a2c064b7",
   "metadata": {},
   "source": [
    "Q1. Explain the properties of the F-distribution. \n",
    "Ans. The F-distribution is a continuous probability distribution that arises frequently in statistical analysis, particularly in analysis of variance \n",
    "     (ANOVA), hypothesis testing, and regression analysis. Here are its key properties:\n",
    "\n",
    "# Properties of the F-Distribution\r\n",
    "\r\n",
    "### 1. Definition\r\n",
    "The F-distribution is the ratio of two scaled chi-squared distributions. If:\r\n",
    "- \\( X \\sim \\chi^2_{d_1} \\)\r\n",
    "- \\( Y \\sim \\chi^2_{d_2} \\)\r\n",
    "\r\n",
    "Then the F-statistic is defined as:\r\n",
    "\\[\r\n",
    "F = \\frac{(X/d_1)}{(Y/d_2)}\r\n",
    "\\]\r\n",
    "This follows an F-distribution with \\( d_1 \\) and \\( d_2 \\) degrees oedom.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. Shape\r\n",
    "- The F-distribution is **positively skewed**.\r\n",
    "- The skewness decreases as \\( d_1 \\) and \\( d_2 \\) increase, approaching symmetry for large values of degof freedom.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. Range\r\n",
    "The F-distribution is defined only for positive values:\r\n",
    "\\\\in [0, \\infty)\r\n",
    "\\]\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 4. Parameters\r\n",
    "The F-distribution is characterized by two parameters:\r\n",
    "- \\( d_1 \\): Degrees of freedom for the numerator.\r\n",
    "- \\( d_2 \\): Degrees eedom for the denominator.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 5. Mean\r\n",
    "The mean of the F-distribution exists only if \\( d_2 > 2 \\), and is given by:\r\n",
    "\\[xt{Mean} = \\frac{d_2}{d_2 - 2}\r\n",
    "\\]\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 6. Variance\r\n",
    "The variance exists only if \\( d_2 > 4 \\), and is given by:\r\n",
    "\\[\r\n",
    "\\text{Variance} = \\frac{2d_21 + d_2 - 2)}{d_1(d_2 - 2)^2(d_2 - 4)}\r\n",
    "\\]\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 7. Relationship to Other Distributions\r\n",
    "- The F-distribution is derived from the ratio of two independent chi-squared variables divided by their respective degrees of freedom.\r\n",
    "- As \\( d_2 \\to \\infty \\), the F-distribution approache chi-squared distribution scaled by \\( d_1 \\).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 8. Applications\r\n",
    "The F-distribution is widely used in:\r\n",
    "1. **ANOVA**: Testing if variances between groups are significantly different.\r\n",
    "2. **Regression Analysis**: Evaluating the overall significance of el.\r\n",
    "3. **Hypothesis Testing**: Comparing two variances.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 9. Cumulative Distribution Function (CDF)\r\n",
    "- The CDF of the F-distribution is used to compute \\( p \\)-values in hypothesis testing.\r\n",
    "- It does not have a simple closed-form expression a typically calculated using statistical software or libraries.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 10. Non-Symmetry\r\n",
    "- The F-distribution is not symme\r\n",
    "- It is more skewed for smaller values of \\( d_1 \\) and \\( d_2 \\).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Summary\r\n",
    "The F-distribution is a critical tool in statistical hypothesis testing, especially for comparing variances and assessing model fit.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facbe81e-8606-427a-9fe3-3d083737b917",
   "metadata": {},
   "source": [
    "Q2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
    "Ans.\n",
    "# Use of the F-Distribution in Statistical Tests\r\n",
    "\r\n",
    "The F-distribution is used in several types of statistical tests. Its primary application lies in comparing variances and assessing the overall fit of models. Below are the main types of statistical tests where it is used, along with reasons why it is appropriate for these tests---\r\n",
    "\r\n",
    "## 1. **Analysis of Variance (ANOVA)**\r\n",
    "- **Purpose**: To test if the means of multiple groups are significantly different.\r\n",
    "- **How it uses the F-distribution**:\r\n",
    "  - ANOVA calculates the ratio of variance between groups (explained variance) to variance within groups (unexplained variance).\r\n",
    "  - This ratio follows the F-distribution under the null hypothesis.\r\n",
    "- **Why it's appropriate**:\r\n",
    "  - The F-distribution is ideal for comparing two variances, which is the corANOVA.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 2. **Regression Analysis**\r\n",
    "- **Purpose**: To test the overall significance of a regression model.\r\n",
    "- **How it uses the F-distribution**:\r\n",
    "  - The F-test compares the explained variance (due to the regression model) with the residual variance (due to random error).\r\n",
    "  - The test determines whether the predictors collectively explain a significant proportion of the variance in the dependent variable.\r\n",
    "- **Why it's appropriate**:\r\n",
    "  - The F-distribution is used because it evaluates the ratio of variances, helpssess model fit.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 3. **Equality of Two Variances**\r\n",
    "- **Purpose**: To compare the variances of two independent samples.\r\n",
    "- **How it uses the F-distribution**:\r\n",
    "  - The test calculates the ratio of the two sample variances.\r\n",
    "  - This ratio follows the F-distribution under the null hypothesis that the variances are equal.\r\n",
    "- **Why it's appropriate**:\r\n",
    "  - The F-distribution specifically models the ratio of variances, making it teural choice for this test.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 4. **Hypothesis Testing in Multivariate Analysis**\r\n",
    "- **Purpose**: To test hypotheses in multivariate techniques like MANOVA, canonical correlation, and discriminant analysis.\r\n",
    "- \r\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8a64fe-ce43-4b20-a96e-72c6648a7613",
   "metadata": {},
   "source": [
    "Q3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?\n",
    "Ans.\n",
    "   # Key Assumptions for Conducting an F-Test to Compare Variances\r\n",
    "\r\n",
    "The F-test is used to compare the variances of two populations. For the test results to be valid, the following assumptions must be me\n",
    "---\r\n",
    "\r\n",
    "### 1. **Normality of Data**\r\n",
    "- The data in both populations must follow a **normal distribution**.\r\n",
    "- The F-test is sensitive to departures from normality, which can affect the validity of the rs.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. **Independence of Observations**\r\n",
    "- The samples must be drawn **independently** from their respective populations.\r\n",
    "- This ensures that the variances being compared are not influenced by a relationship betweensamples.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. **Random Sampling**\r\n",
    "- The data must be collected through **random sampling**.\r\n",
    "- This minimizes bias and ensures that the samples are representative ofr populations.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 4. **Scale of Measurement**\r\n",
    "- The variables being analyzed must be measured on a **continuous scale** (e.g., height, weight, temperature).\r\n",
    "- This assumption ensures that variances are mgful and comparable.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 5. **Homogeneity of Variances (Only for ANOVA)**\r\n",
    "- In cases where the F-test is used as part of ANOVA, an additional assumption of **equal variances across groups** is made.\r\n",
    "- However, this is not required for the F-test speci\n",
    "- y comparing two variances.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 6. **Non-Negative Variances**\r\n",
    "- Variances being compared must be **non-negative**, as variannnot be negative by definition.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Summary\r\n",
    "Violations of these assumptions can lead to incorrect conclusions. If normality is questionable, non-parametric alternatives like the Levene's test or Bartlett's test should be considered.\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc52e6-0642-411b-8c55-ed7a4e66c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q-4. What is the purpose of ANOVA, and how does it differ from a t-test? \n",
    "Ans.\n",
    "# Purpose of ANOVA and How it Differs from a t-Test\n",
    "\n",
    "## 1. **Purpose of ANOVA**\n",
    "- **ANOVA (Analysis of Variance)** is a statistical method used to determine if there are significant differences between the means of **three or more groups**.\n",
    "- It analyzes the variability in the data by comparing:\n",
    "  - **Between-group variance** (variability due to differences between group means).\n",
    "  - **Within-group variance** (variability due to differences within each group).\n",
    "- **Null Hypothesis (\\(H_0\\))**: All group means are equal.\n",
    "- **Alternative Hypothesis (\\(H_a\\))**: At least one group mean is significantly different.\n",
    "\n",
    "## 2. **Key Applications of ANOVA**\n",
    "- Comparing test scores of students across multiple schools.\n",
    "- Testing the effectiveness of different medications across patient groups.\n",
    "- Analyzing the impact of different marketing strategies on sales.\n",
    "\n",
    "## 3. **Difference Between ANOVA and t-Test**\n",
    "\n",
    "| **Aspect**                | **t-Test**                                   | **ANOVA**                                       |\n",
    "|---------------------------|---------------------------------------------|-----------------------------------------------|\n",
    "| **Number of Groups**       | Compares the means of **two groups**.       | Compares the means of **three or more groups**.|\n",
    "| **Purpose**                | Tests if the means of two groups are equal. | Tests if at least one group mean is different.|\n",
    "| **Statistical Output**     | Produces a **t-statistic**.                 | Produces an **F-statistic**.                  |\n",
    "| **Type of Variance**       | Compares variances between two groups.      | Compares **between-group** and **within-group** variances. |\n",
    "| **Error Rate**             | Repeated t-tests increase the **Type I error** rate. | ANOVA controls the **Type I error** rate when comparing multiple groups. |\n",
    "| **Post-Hoc Tests**         | Not required.                              | Requires post-hoc tests (e.g., Tukey's test) to identify which groups differ. |\n",
    "\n",
    "## 4. **Why Use ANOVA Instead of t-Test for Multiple Groups?**\n",
    "1. **Control Type I Error**:\n",
    "   - Conducting multiple t-tests increases the likelihood of falsely rejecting the null hypothesis.\n",
    "   - ANOVA maintains the overall error rate at the desired significance level.\n",
    "   \n",
    "2. **Efficiency**:\n",
    "   - ANOVA evaluates all groups simultaneously, making it more efficient than conducting multiple t-tests.\n",
    "\n",
    "3. **Comprehensive Analysis**:\n",
    "   - ANOVA provides a single test to assess whether there is any difference among group means, without needing to perform pairwise comparisons upfront.\n",
    "\n",
    "## 5. **Post-Hoc Analysis**\n",
    "- If ANOVA indicates significant differences, post-hoc tests (e.g., Tukey's HSD or Bonferroni correction) are performed to identify which specific groups differ.\n",
    "\n",
    "### Summary\n",
    "- Use a **t-test** for comparing **two groups**.\n",
    "- Use **ANOVA** for comparing **three or more groups** to control error rates and obtain a more efficient analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621640cd-0d30-4379-9b3e-d6d4a6ff6250",
   "metadata": {},
   "source": [
    "Q5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.\n",
    "Ans.\n",
    "    # When and Why to Use One-Way ANOVA Instead of Multiple t-Tests\n",
    "\n",
    "## 1. **When to Use One-Way ANOVA**\n",
    "- Use **one-way ANOVA** when:\n",
    "  - You need to compare the means of **three or more independent groups**.\n",
    "  - The groups are categorized based on a **single factor** (e.g., different diets, teaching methods, or treatments).\n",
    "  - The goal is to test whether there is a significant difference among the group means.\n",
    "\n",
    "## 2. **Why One-Way ANOVA is Preferred Over Multiple t-Tests**\n",
    "\n",
    "### **2.1. Controls Type I Error**\n",
    "- **Type I Error** occurs when you incorrectly reject the null hypothesis (\\(H_0\\)) when it is true.\n",
    "- With multiple t-tests:\n",
    "  - Each t-test is conducted at a fixed significance level (\\( \\alpha \\)), such as 0.05.\n",
    "  - The overall error rate increases as more t-tests are performed, calculated as:\n",
    "    \\[\n",
    "    \\text{Overall Error Rate} = 1 - (1 - \\alpha)^k\n",
    "    \\]\n",
    "    where \\( k \\) is the number of comparisons.\n",
    "  - For example, comparing 4 groups requires \\( k = 6 \\) t-tests, resulting in a much higher error rate.\n",
    "- **One-way ANOVA** conducts a single test, maintaining the significance level (\\( \\alpha \\)) at the desired value (e.g., 0.05).\n",
    "\n",
    "### **2.2. Efficiency**\n",
    "- **Multiple t-tests** require pairwise comparisons for all groups:\n",
    "  - For \\( g \\) groups, the number of comparisons is:\n",
    "    \\[\n",
    "    k = \\frac{g(g - 1)}{2}\n",
    "    \\]\n",
    "  - For 4 groups, this means 6 t-tests; for 6 groups, it means 15 t-tests.\n",
    "- **One-way ANOVA** evaluates all groups simultaneously in a single analysis, saving time and computational effort.\n",
    "\n",
    "### **2.3. Comprehensive Analysis**\n",
    "- **t-tests** only evaluate differences between pairs of groups.\n",
    "- **One-way ANOVA** tests the overall null hypothesis (\\(H_0\\)):\n",
    "  \\[\n",
    "  H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\ldots = \\mu_g\n",
    "  \\]\n",
    "  - It determines whether **any significant difference exists** among the group means.\n",
    "  - If ANOVA indicates significance, post-hoc tests (e.g., Tukey’s HSD) can pinpoint specific group differences.\n",
    "\n",
    "### **2.4. Validity**\n",
    "- **One-way ANOVA** is specifically designed for comparing more than two groups while assuming:\n",
    "  1. Independent observations.\n",
    "  2. Normally distributed data within groups.\n",
    "  3. Equal variances (homogeneity of variance).\n",
    "- These assumptions allow ANOVA to handle more complex group comparisons accurately.\n",
    "\n",
    "## 3. **Example**\n",
    "### Scenario:\n",
    "- A researcher tests the effect of three diets on weight loss.\n",
    "- Groups:\n",
    "  - Group 1: Diet A\n",
    "  - Group 2: Diet B\n",
    "  - Group 3: Diet C\n",
    "\n",
    "#### Options:\n",
    "- Conducting multiple t-tests:\n",
    "  - Compare Diet A vs. Diet B, Diet A vs. Diet C, Diet B vs. Diet C.\n",
    "  - Results in \\( k = 3 \\) tests, increasing the risk of Type I error.\n",
    "- Using one-way ANOVA:\n",
    "  - Conduct a single test to determine if any diet leads to a significant difference in weight loss.\n",
    "\n",
    "## 4. **Summary**\n",
    "- Use **one-way ANOVA** instead of multiple t-tests when comparing **three or more groups** to:\n",
    "  1. Control the overall Type I error rate.\n",
    "  2. Simplify and streamline the analysis.\n",
    "  3. Perform a comprehensive test to detect significant differences across all groups.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af77e55-c93a-4e86-8add-d9fb9f8ed41b",
   "metadata": {},
   "source": [
    "Q6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the \n",
    "    calculation of the F-statistic?\n",
    "Ans.\n",
    "# Variance Partitioning in ANOVA and Its Role in Calculating the F-Statistic\n",
    "\n",
    "## 1. **Understanding Variance in ANOVA**\n",
    "In ANOVA, the total variability in the data is divided into two components:\n",
    "1. **Between-Group Variance**: Variability due to differences between the group means.\n",
    "2. **Within-Group Variance**: Variability due to differences within each group (individual differences or random error).\n",
    "\n",
    "The total variance is mathematically expressed as:\n",
    "\\[\n",
    "\\text{Total Variance} = \\text{Between-Group Variance} + \\text{Within-Group Variance}\n",
    "\\]\n",
    "\n",
    "## 2. **Partitioning Variance**\n",
    "### **2.1. Total Sum of Squares (\\(SS_{total}\\))**\n",
    "- Measures the overall variability in the data.\n",
    "- Formula:\n",
    "  \\[\n",
    "  SS_{total} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "  \\]\n",
    "  where:\n",
    "  - \\( y_i \\): Individual data points.\n",
    "  - \\( \\bar{y} \\): Grand mean (mean of all observations).\n",
    "\n",
    "### **2.2. Between-Group Sum of Squares (\\(SS_{between}\\))**\n",
    "- Measures variability due to differences between group means.\n",
    "- Formula:\n",
    "  \\[\n",
    "  SS_{between} = \\sum_{j=1}^{k} n_j (\\bar{y}_j - \\bar{y})^2\n",
    "  \\]\n",
    "  where:\n",
    "  - \\( n_j \\): Number of observations in group \\( j \\).\n",
    "  - \\( \\bar{y}_j \\): Mean of group \\( j \\).\n",
    "  - \\( \\bar{y} \\): Grand mean.\n",
    "\n",
    "### **2.3. Within-Group Sum of Squares (\\(SS_{within}\\))**\n",
    "- Measures variability within each group (due to random error or individual differences).\n",
    "- Formula:\n",
    "  \\[\n",
    "  SS_{within} = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (y_{ij} - \\bar{y}_j)^2\n",
    "  \\]\n",
    "  where:\n",
    "  - \\( y_{ij} \\): Individual data point in group \\( j \\).\n",
    "  - \\( \\bar{y}_j \\): Mean of group \\( j \\).\n",
    "\n",
    "## 3. **Relationship Between Variances**\n",
    "The total variability is the sum of between-group and within-group variability:\n",
    "\\[\n",
    "SS_{total} = SS_{between} + SS_{within}\n",
    "\\]\n",
    "\n",
    "## 4. **Calculating the F-Statistic**\n",
    "The F-statistic is the ratio of the **between-group variance** to the **within-group variance**:\n",
    "\\[\n",
    "F = \\frac{\\text{Variance Between Groups}}{\\text{Variance Within Groups}}\n",
    "\\]\n",
    "\n",
    "### **4.1. Mean Squares**\n",
    "- Variances are calculated as **mean squares** (sum of squares divided by corresponding degrees of freedom):\n",
    "  - **Between-Group Mean Square (\\(MS_{between}\\))**:\n",
    "    \\[\n",
    "    MS_{between} = \\frac{SS_{between}}{df_{between}}\n",
    "    \\]\n",
    "    where \\( df_{between} = k - 1 \\) (number of groups minus 1).\n",
    "  - **Within-Group Mean Square (\\(MS_{within}\\))**:\n",
    "    \\[\n",
    "    MS_{within} = \\frac{SS_{within}}{df_{within}}\n",
    "    \\]\n",
    "    where \\( df_{within} = N - k \\) (total number of observations minus number of groups).\n",
    "\n",
    "### **4.2. F-Statistic Formula**\n",
    "\\[\n",
    "F = \\frac{MS_{between}}{MS_{within}}\n",
    "\\]\n",
    "\n",
    "## 5. **Role of Partitioning in ANOVA**\n",
    "1. **Between-Group Variance**:\n",
    "   - Captures the systematic variation due to group differences.\n",
    "   - If the groups have distinct means, \\( MS_{between} \\) will be large.\n",
    "\n",
    "2. **Within-Group Variance**:\n",
    "   - Captures the random variation within each group.\n",
    "   - Acts as a baseline to compare the between-group variance.\n",
    "\n",
    "3. **F-Statistic**:\n",
    "   - A high \\( F \\)-statistic indicates that the between-group variance is significantly larger than the within-group variance, suggesting that the \n",
    "    group means are not equal.\n",
    "\n",
    "## 6. **Decision Rule**\n",
    "- Compare the calculated \\( F \\)-statistic to the critical value from the F-distribution (based on \\( df_{between} \\) and \\( df_{within} \\)).\n",
    "- If \\( F \\)-statistic is larger than the critical value, reject the null hypothesis (\\( H_0 \\)) that all group means are equal.\n",
    "\n",
    "### Summary\n",
    "The partitioning of variance in ANOVA allows us to separate the effect of group differences (systematic variation) from random error (unsystematic\n",
    "variation). The F-statistic, derived from this partitioning, helps determine whether the group means are significantly different.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527070a6-45ff-4ac0-90de-6f3cc849ae06",
   "metadata": {},
   "source": [
    "Q7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle\n",
    "   uncertainty, parameter estimation, and hypothesis testing?\n",
    "Ans.\n",
    "# Comparison of the Classical (Frequentist) and Bayesian Approaches to ANOVA\n",
    "\n",
    "## 1. **Classical (Frequentist) ANOVA**\n",
    "\n",
    "### **1.1. Key Features**\n",
    "- Relies on frequentist principles and long-run probabilities.\n",
    "- Assumes fixed model parameters (e.g., group means and variances).\n",
    "- Hypothesis testing is conducted using \\( p \\)-values and F-statistics.\n",
    "\n",
    "### **1.2. How It Handles Uncertainty**\n",
    "- Uncertainty is expressed in terms of **sampling variability**.\n",
    "- Uses the null hypothesis (\\( H_0 \\)) as the baseline assumption (e.g., all group means are equal).\n",
    "- The \\( p \\)-value quantifies the probability of observing data as extreme as (or more extreme than) the observed, assuming \\( H_0 \\) is true.\n",
    "\n",
    "### **1.3. Parameter Estimation**\n",
    "- Estimates parameters like group means and variances using **maximum likelihood estimation (MLE)**.\n",
    "- Produces point estimates for parameters without incorporating prior information.\n",
    "\n",
    "### **1.4. Hypothesis Testing**\n",
    "- Relies on the F-statistic to compare between-group and within-group variances.\n",
    "- Tests \\( H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_k \\) (all group means are equal) against the alternative hypothesis \\( H_a \\) (at least one mean is different).\n",
    "- Decisions are based on whether the \\( p \\)-value is below a predetermined significance level (e.g., 0.05).\n",
    "\n",
    "## 2. **Bayesian ANOVA**\n",
    "\n",
    "### **2.1. Key Features**\n",
    "- Based on Bayesian principles, which combine prior beliefs with observed data.\n",
    "- Treats parameters (e.g., group means and variances) as random variables with probability distributions.\n",
    "- Provides a probabilistic framework for inference.\n",
    "\n",
    "### **2.2. How It Handles Uncertainty**\n",
    "- Models uncertainty explicitly using **posterior distributions**.\n",
    "- The posterior distribution is calculated using Bayes' theorem:\n",
    "  \\[\n",
    "  P(\\text{parameters} \\mid \\text{data}) = \\frac{P(\\text{data} \\mid \\text{parameters}) P(\\text{parameters})}{P(\\text{data})}\n",
    "  \\]\n",
    "  - **Prior** (\\( P(\\text{parameters}) \\)): Initial beliefs about parameters before observing data.\n",
    "  - **Likelihood** (\\( P(\\text{data} \\mid \\text{parameters}) \\)): Information provided by the observed data.\n",
    "  - **Posterior** (\\( P(\\text{parameters} \\mid \\text{data}) \\)): Updated beliefs after observing data.\n",
    "\n",
    "### **2.3. Parameter Estimation**\n",
    "- Produces **posterior distributions** for parameters rather than point estimates.\n",
    "- Can incorporate prior information, improving estimation when data is scarce or noisy.\n",
    "\n",
    "### **2.4. Hypothesis Testing**\n",
    "- Instead of \\( p \\)-values, evaluates **posterior probabilities** of hypotheses.\n",
    "- Compares models or hypotheses using metrics like:\n",
    "  - **Bayes Factors (BF)**: Quantifies evidence for one hypothesis relative to another.\n",
    "  - **Credible Intervals**: Bayesian analog of confidence intervals, showing the range where parameters lie with a certain probability (e.g., 95%).\n",
    "\n",
    "## 3. **Key Differences Between Frequentist and Bayesian ANOVA**\n",
    "\n",
    "| **Aspect**                  | **Frequentist ANOVA**                          | **Bayesian ANOVA**                               |\n",
    "|-----------------------------|-----------------------------------------------|------------------------------------------------|\n",
    "| **View of Parameters**       | Fixed values to be estimated.                 | Random variables with probability distributions. |\n",
    "| **Uncertainty**              | Expressed via \\( p \\)-values and confidence intervals. | Expressed via posterior distributions and credible intervals. |\n",
    "| **Role of Prior Knowledge**  | Does not incorporate prior information.       | Explicitly incorporates prior knowledge.        |\n",
    "| **Inference**                | Based on rejection of null hypothesis.        | Based on posterior probabilities and Bayes Factors. |\n",
    "| **Output**                   | Point estimates and significance tests.       | Full probability distributions for parameters.  |\n",
    "| **Flexibility**              | Assumes fixed model structure.                | Allows for more flexible model structures.      |\n",
    "| **Interpretation of Results**| Probabilistic statements about data given \\( H_0 \\). | Probabilistic statements about parameters given the data. |\n",
    "\n",
    "## 4. **Example of Interpretation**\n",
    "- **Frequentist**: \"The \\( p \\)-value is 0.03, so we reject the null hypothesis and conclude that at least one group mean differs.\"\n",
    "- **Bayesian**: \"The posterior probability that at least one group mean differs is 0.97. The Bayes Factor provides strong evidence in favor of this conclusion.\"\n",
    "\n",
    "## 5. **When to Use Each Approach**\n",
    "- **Frequentist ANOVA**:\n",
    "  - When prior information is unavailable or unnecessary.\n",
    "  - For quick hypothesis testing with widely accepted methods.\n",
    "  - When computational resources are limited.\n",
    "- **Bayesian ANOVA**:\n",
    "  - When prior information is available or desired.\n",
    "  - For situations requiring detailed probabilistic inference.\n",
    "  - When handling small datasets or complex models.\n",
    "\n",
    "### Summary\n",
    "Frequentist ANOVA focuses on hypothesis testing using \\( p \\)-values, while Bayesian ANOVA provides a flexible framework for modeling uncertainty and \n",
    "    incorporating prior information. The choice between them depends on the context, goals, and available resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1075f207-3fdc-4783-89e5-afc10ce80958",
   "metadata": {},
   "source": [
    "Q8. Question: You have two sets of data representing the incomes of two different professions1\n",
    "   - Profession A: [48, 52, 55, 60, 62'\n",
    "   - Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions' incomes are equal. What are your              conclusions based on the F-test?\n",
    " \n",
    "   Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
    "Ans.\n",
    "        import numpy as np\r",
    "        \n",
    "from scipy.stats import f        \r",
    "        \n",
    "\r\n",
    "# Data for the two professio        ns\r\n",
    "profession_a = np.array([48, 52, 55, 60, 6        2])\r\n",
    "profession_b = np.array([45, 50, 55, 52,         4        7])\r\n",
    "\r\n",
    "# Calculate variances of the two s        amples\r\n",
    "var_a = np.var(profession_a, ddof=1)  # Sample variance of Profe        ssion A\r\n",
    "var_b = np.var(profession_b, ddof=1)  # Sample variance of Prof        e        ssion B\r\n",
    "\r\n",
    "# Calculate the F        -statistic\r\n",
    "f_statistic = va        r        _a / var_b\r\n",
    "\r\n",
    "# Degrees of freedom for th        e two samples\r\n",
    "df_a = len(pro        fession_a) - 1\r\n",
    "df_b = len(pr        o        fession_b) - 1\r\n",
    "\r\n",
    "# Calculate the p-value using t        he F-distribution\r\n",
    "        p_value = 2 * min(\r\n",
    "    f.cdf(f_statistic, df_a, d        f_b),  # Lower tail\r\n",
    "    1 - f.cdf(f_statistic, df_a,         d        f        _b)  # Upper tail\r\n",
    ")\r\n",
    "        Result\n",
    "        (3.232989690721649, 0.10987970118946545)\n",
    "\r\n",
    "f_statistic, p_value\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16898c63-35fe-4046-82d6-106f5c1b5a06",
   "metadata": {},
   "source": [
    "Q9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different\n",
    "     regions with the following data:\n",
    " --  Region A: [160, 162, 165, 158, 164'\r",
    "V--   Region B: [172, 175, 170, 168, 174'\n",
    "--  V Region C: [180, 182, 179, 185, 183\r",
    "--  \n",
    "V Task: Write Python code to perform the one-way ANOVA and interpret the resultsAns.\n",
    "        # Data for the heights in three different regions\r",
    "        \n",
    "region_a = np.array([160, 162, 165, 158, 164])        \r\n",
    "region_b = np.array([172, 175, 170, 168, 174]        )\r\n",
    "region_c = np.array([180, 182, 179, 185, 183        ]        )\r\n",
    "\r\n",
    "# Perform one-way A        NOVA\r\n",
    "f_statistic, p_value = f_oneway(region_a, region_b, regi        o        n_c)\r\n",
    "\r\n",
    "f_statistic, \n",
    "        Result\n",
    "       (67.87330316742101, 2.870664187937026e-07)p_value\r\n",
    "  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
